export default {
  async fetch(request, env, ctx) {
    if (request.method === 'GET') {
      return new Response(`<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«</title>
  <style>
    body { font-family: sans-serif; max-width: 700px; margin: 2rem auto; padding: 1rem; background: #f5f5f5; }
    button { font-size: 1.1rem; padding: 0.6rem 1rem; margin-right: 1rem; background: #0078d4; color: white; border: none; border-radius: 6px; cursor: pointer; }
    button:disabled { background: #aaa; }
    #result > div { background: white; margin-top: 1rem; padding: 0.8rem; border-radius: 6px; }
  </style>
</head>
<body>
  <h1>ğŸ¤ è‡ªåŠ¨è¿ç»­è¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰</h1>
  <p>ç‚¹å‡»â€œå¼€å§‹â€åï¼Œæ¯æ¬¡è¯´è¯åï¼š<br>
  - å¦‚æœé™éŸ³ 5 ç§’ï¼Œæˆ–<br>
  - è¯´å‡ºâ€œok okâ€ï¼Œæ²¡æœ‰ç”¨ã€‚è¿™ä¸ªåŠŸèƒ½ã€‚å°†è‡ªåŠ¨è§¦å‘è¯†åˆ«å¹¶è¿›å…¥ä¸‹ä¸€è½®</p>
  <button id="start-btn">â–¶ï¸ å¼€å§‹è¯†åˆ«</button>
  <button id="stop-btn" disabled>â¹ï¸ åœæ­¢</button>
  <div id="result">ğŸ“ è¯†åˆ«ç»“æœï¼š</div>

<script>
let isRunning = false;
let counter = 1;

const startBtn = document.getElementById('start-btn');
const stopBtn = document.getElementById('stop-btn');
const result = document.getElementById('result');

startBtn.onclick = () => {
  if (!isRunning) {
    isRunning = true;
    startBtn.disabled = true;
    stopBtn.disabled = false;
    startLoop();
  }
};

stopBtn.onclick = () => {
  isRunning = false;
  startBtn.disabled = false;
  stopBtn.disabled = true;
};

async function startLoop() {
  while (isRunning) {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const recorder = new MediaRecorder(stream);
    const chunks = [];

    recorder.ondataavailable = e => chunks.push(e.data);
    recorder.start();

    const audioCtx = new AudioContext();
    const source = audioCtx.createMediaStreamSource(stream);
    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 2048;
    source.connect(analyser);
    const data = new Uint8Array(analyser.fftSize);

    let silentFrames = 0;
    const silenceThreshold = 8;
    const maxSilentFrames = 150; // â‰ˆ5ç§’

    await new Promise(resolve => {
      function detectSilence() {
        analyser.getByteTimeDomainData(data);
        const avg = data.reduce((a, b) => a + Math.abs(b - 128), 0) / data.length;
        if (avg < silenceThreshold) {
          silentFrames++;
          if (silentFrames > maxSilentFrames) {
            recorder.stop();
            audioCtx.close();
            stream.getTracks().forEach(track => track.stop());
            resolve();
            return;
          }
        } else {
          silentFrames = 0;
        }

        if (!isRunning) {
          recorder.stop();
          audioCtx.close();
          stream.getTracks().forEach(track => track.stop());
          resolve();
          return;
        }

        requestAnimationFrame(detectSilence);
      }
      detectSilence();
    });

    await new Promise(resolve => {
      recorder.onstop = async () => {
        const blob = new Blob(chunks, { type: 'audio/webm' });
        const formData = new FormData();
        formData.append('audio', blob, 'audio.webm');

        const time = new Date().toLocaleTimeString();
        const div = document.createElement('div');
        div.textContent = \`#\${counter} ğŸ•’ \${time} â³ æ­£åœ¨è¯†åˆ«...\`;
        result.appendChild(div);

        try {
          const res = await fetch('/', { method: 'POST', body: formData });
          const json = await res.json();
          const text = json.transcription || 'âš ï¸ æ— è¯†åˆ«å†…å®¹';
          div.textContent = \`#\${counter} ğŸ•’ \${time} ğŸ“ \${text}\`;
          counter++;

          // å¦‚æœè¯´äº† ok okï¼Œè‡ªåŠ¨åœæ­¢è¯†åˆ«
const stopWords = ['ok', 'okay', 'okok', 'ã‚ªãƒ¼ã‚±ãƒ¼', 'ï¼¯ï¼«', 'ç»“æŸ', 'stop'];
if (stopWords.some(word => text.toLowerCase().includes(word))) {
  isRunning = false;
  startBtn.disabled = false;
  stopBtn.disabled = true;
}

        } catch (err) {
          div.textContent = \`#\${counter} âŒ é”™è¯¯: \${err.message}\`;
        }

        resolve();
      };
    });

    await new Promise(r => setTimeout(r, 800)); // å°é—´éš”é˜²æŠ–
  }
}
</script>
</body>
</html>`, {
        headers: { 'Content-Type': 'text/html; charset=UTF-8' }
      });
    }

    if (request.method === 'POST') {
      try {
        const formData = await request.formData();
        const audioFile = formData.get('audio');
        if (!audioFile) {
          return new Response(JSON.stringify({ error: 'ç¼ºå°‘éŸ³é¢‘æ–‡ä»¶' }), {
            status: 400,
            headers: { 'Content-Type': 'application/json' }
          });
        }

        const buffer = await audioFile.arrayBuffer();
        const audioBytes = [...new Uint8Array(buffer)];

        const result = await env.AI.run('@cf/openai/whisper', {
          audio: audioBytes
        });

        return new Response(JSON.stringify({ transcription: result.text }), {
          headers: { 'Content-Type': 'application/json' }
        });
      } catch (err) {
        return new Response(JSON.stringify({ error: err.message }), {
          status: 500,
          headers: { 'Content-Type': 'application/json' }
        });
      }
    }

    return new Response('Method Not Allowed', { status: 405 });
  }
}
